{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Experiments on Clean and Noisy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification on IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv(\"../datasets/clean/imdb.csv\")\n",
    "imdb_df['sentiment'] = imdb_df['sentiment'].apply(lambda x: 1 if x =='positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(imdb_df.loc[:25000, 'review'])\n",
    "y_train = np.array(imdb_df.loc[:25000, 'sentiment'])\n",
    "x_test = np.array(imdb_df.loc[25001:, 'review'])\n",
    "y_test = np.array(imdb_df.loc[25001:, 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text classifier.\n",
    "clf = ak.TextClassifier(overwrite=True, max_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 01s]\n",
      "\n",
      "Best val_loss So Far: 0.3015161454677582\n",
      "Total elapsed time: 00h 01m 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 32s 40ms/step - loss: 0.4450 - accuracy: 0.7698\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.2420 - accuracy: 0.9033\n",
      "INFO:tensorflow:Assets written to: ./text_classifier/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./text_classifier/best_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 12ms/step\n",
      "782/782 [==============================] - 9s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "# Feed the text classifier with training data.\n",
    "clf.fit(x_train, y_train, epochs=2)\n",
    "# Predict with the best model.\n",
    "predicted_y = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 9s 12ms/step - loss: 0.2813 - accuracy: 0.8854\n",
      "[0.2813020646572113, 0.8854354023933411]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv(\"../datasets/noisy/imdb_noisy.csv\")\n",
    "imdb_df['sentiment'] = imdb_df['sentiment'].apply(lambda x: 1 if x =='positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(imdb_df.loc[:25000, 'review'])\n",
    "y_train = np.array(imdb_df.loc[:25000, 'sentiment'])\n",
    "x_test = np.array(imdb_df.loc[25001:, 'review'])\n",
    "y_test = np.array(imdb_df.loc[25001:, 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text classifier.\n",
    "clf = ak.TextClassifier(overwrite=True, max_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 00s]\n",
      "\n",
      "Best val_loss So Far: 0.42938950657844543\n",
      "Total elapsed time: 00h 01m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5434 - accuracy: 0.7044\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.3653 - accuracy: 0.8400\n",
      "INFO:tensorflow:Assets written to: ./text_classifier/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./text_classifier/best_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 9s 12ms/step\n",
      "782/782 [==============================] - 9s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "# Feed the text classifier with training data.\n",
    "clf.fit(x_train, y_train, epochs=2)\n",
    "# Predict with the best model.\n",
    "predicted_y = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 12ms/step - loss: 0.3968 - accuracy: 0.8226\n",
      "[0.3968028724193573, 0.8225529193878174]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification on IRIS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tpot.export_utils import set_param_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "iris_df = pd.read_csv(\"../datasets/clean/iris.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df.loc[:, iris_df.columns != 'target'], \n",
    "                                                    iris_df.loc[:, 'target'], \n",
    "                                                    train_size=0.75, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc65675cca0646cc8baf541ab868f069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 1.0\n",
      "\n",
      "Generation 2 - Current best internal CV score: 1.0\n",
      "\n",
      "Generation 3 - Current best internal CV score: 1.0\n",
      "\n",
      "Generation 4 - Current best internal CV score: 1.0\n",
      "\n",
      "Generation 5 - Current best internal CV score: 1.0\n",
      "\n",
      "Best pipeline: KNeighborsClassifier(input_matrix, n_neighbors=3, p=1, weights=distance)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('best_model_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average CV score on the training set was: 0.9826086956521738\n",
    "exported_pipeline = make_pipeline(\n",
    "    Normalizer(norm=\"l2\"),\n",
    "    KNeighborsClassifier(n_neighbors=3, p=1, weights=\"distance\")\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "results = exported_pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
